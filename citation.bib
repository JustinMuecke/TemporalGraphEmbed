@article{DBLP:journals/corr/abs-1905-05304,
  author    = {George B. Mertzios and
               Hendrik Molter and
               Rolf Niedermeier and
               Viktor Zamaraev and
               Philipp Zschoche},
  title     = {Computing Maximum Matchings in Temporal Graphs},
  journal   = {CoRR},
  volume    = {abs/1905.05304},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.05304},
  archivePrefix = {arXiv},
  eprint    = {1905.05304},
  timestamp = {Tue, 28 May 2019 12:48:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-05304.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{gao2018properties,
      title={On the Properties of the Softmax Function with Application in Game Theory and Reinforcement Learning}, 
      author={Bolin Gao and Lacra Pavel},
      year={2018},
      eprint={1704.00805},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{DBLP:journals/corr/NarayananCVCLJ17,
  author    = {Annamalai Narayanan and
               Mahinthan Chandramohan and
               Rajasekar Venkatesan and
               Lihui Chen and
               Yang Liu and
               Shantanu Jaiswal},
  title     = {graph2vec: Learning Distributed Representations of Graphs},
  journal   = {CoRR},
  volume    = {abs/1707.05005},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.05005},
  archivePrefix = {arXiv},
  eprint    = {1707.05005},
  timestamp = {Mon, 15 Jul 2019 14:17:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/NarayananCVCLJ17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% This file was created with Citavi 6.7.0.0
@article{Godec.31.12.2018,
 abstract = {Graphs are commonly used in different real-world applications. Social networks are large graphs of people that follow each other, biologists use graphs of protein interactions, while communication$\ldots$},
 author = {Godec, Primo{\v{z}}},
 year = {31.12.2018},
 title = {Graph Embeddings --- The Summary - Towards Data Science},
 url = {https://towardsdatascience.com/graph-embeddings-the-summary-cc6075aba007},
 urldate = {07.01.2021},
 journal = {Towards Data Science},
 file = {Godec 31.12.2018 - Graph Embeddings - The Summary:C\:\\Users\\Doo5i\\Documents\\Citavi 6\\Projects\\TemporalGraphEmbedding\\Citavi Attachments\\Godec 31.12.2018 - Graph Embeddings - The Summary.pdf:pdf}
}

@misc{Grover.03.07.2016,
 abstract = {Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.},
 author = {Grover, Aditya and Leskovec, Jure},
 date = {03.07.2016},
 title = {node2vec: Scalable Feature Learning for Networks},
 url = {https://arxiv.org/pdf/1607.00653},
 file = {Grover, Leskovec 03.07.2016 - node2vec Scalable Feature Learning:C\:\\Users\\Doo5i\\Documents\\Citavi 6\\Projects\\TemporalGraphEmbedding\\Citavi Attachments\\Grover, Leskovec 03.07.2016 - node2vec Scalable Feature Learning.pdf:pdf}
}

@proceedings{.2020,
 year = {2020},
 publisher = {ACM},
 isbn = {9781450368599}
}

@inproceedings{Beladev.2020,
 author = {Beladev, Moran and Rokach, Lior and Katz, Gilad and Guy, Ido and Radinsky, Kira},
 title = {tdGraphEmbed: Temporal Dynamic Graph-Level Embedding},
 publisher = {ACM},
 isbn = {9781450368599},
 year = {2020},
 doi = {10.1145/3340531.3411953}
}

% This file was created with Citavi 6.7.0.0
@article{Singer.2019,
 abstract = {In this work, we present a method for node embedding in temporal graphs. We propose an algorithm that learns the evolution of a temporal graph's nodes and edges over time and incorporates this dynamics in a temporal node embedding framework for different graph prediction tasks. We present a joint loss function that creates a temporal embedding of a node by learning to combine its historical temporal embeddings, such that it optimizes per given task (e.g., link prediction). The algorithm is initialized using static node embeddings, which are then aligned over the representations of a node at different time points, and eventually adapted for the given task in a joint optimization. We evaluate the effectiveness of our approach over a variety of temporal graphs for the two fundamental tasks of temporal link prediction and multi-label node classification, comparing to competitive baselines and algorithmic alternatives. Our algorithm shows performance improvements across many of the datasets and baselines and is found particularly effective for graphs that are less cohesive, with a lower clustering coefficient.},
 author = {Singer, Uriel and Guy, Ido and Radinsky, Kira},
 year = {2019},
 title = {Node Embedding over Temporal Graphs},
 url = {https://arxiv.org/pdf/1903.08889},
 pages = {4605--4612},
 journal = {IJCAI},
 doi = {10.24963/ijcai.2019/640},
 file = {Singer, Guy et al. 2019 - Node Embedding over Temporal Graphs:C\:\\Users\\Doo5i\\Documents\\Citavi 6\\Projects\\TemporalGraphEmbedding\\Citavi Attachments\\Singer, Guy et al. 2019 - Node Embedding over Temporal Graphs.pdf:pdf}
}

% This file was created with Citavi 6.7.0.0
@article{Schonemann.1966,
 abstract = {A solutionT of the least-squares problemAT=B +E, givenA andB so that trace (E$\prime$E)= minimum andT$\prime$T=I is presented. It is compared with a less general solution of the same problem which was given by Green [5]. The present solution, in contrast to Green's, is applicable to matricesA andB which are of less than full column rank. Some technical suggestions for the numerical computation ofT and an illustrative example are given.},
 author = {Sch{\"o}nemann, Peter H.},
 year = {1966},
 title = {A generalized solution of the orthogonal procrustes problem},
 pages = {1--10},
 volume = {31},
 number = {1},
 issn = {1860-0980},
 journal = {Psychometrika},
 doi = {10.1007/BF02289451},
 file = {Schönemann 1966 - A generalized solution:C\:\\Users\\Doo5i\\Documents\\Citavi 6\\Projects\\TemporalGraphEmbedding\\Citavi Attachments\\Schönemann 1966 - A generalized solution.pdf:pdf}
}

@misc{Ruder.15.09.2016,
 abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
 author = {Ruder, Sebastian},
 date = {15.09.2016},
 title = {An overview of gradient descent optimization algorithms},
 url = {https://arxiv.org/pdf/1609.04747},
 file = {Ruder 15.09.2016 - An overview of gradient descent:C\:\\Users\\Doo5i\\Documents\\Citavi 6\\Projects\\TemporalGraphEmbedding\\Citavi Attachments\\Ruder 15.09.2016 - An overview of gradient descent.pdf:pdf}
}

@misc{Mikolov.16.01.2013,
 abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
 author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
 date = {16.01.2013},
 title = {Efficient Estimation of Word Representations in Vector Space},
 url = {https://arxiv.org/pdf/1301.3781},
 file = {Mikolov, Chen et al. 16.01.2013 - Efficient Estimation of Word Representations:C\:\\Users\\Doo5i\\Documents\\Citavi 6\\Projects\\TemporalGraphEmbedding\\Citavi Attachments\\Mikolov, Chen et al. 16.01.2013 - Efficient Estimation of Word Representations.pdf:pdf}
}

@misc{Goldberg.15.02.2014,
 abstract = {The word2vec software of Tomas Mikolov and colleagues (https://code.google.com/p/word2vec/ ) has gained a lot of traction lately, and provides state-of-the-art word embeddings. The learning models behind the software are described in two research papers. We found the description of the models in these papers to be somewhat cryptic and hard to follow. While the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind the equations.  This note is an attempt to explain equation (4) (negative sampling) in {\textquotedbl}Distributed Representations of Words and Phrases and their Compositionality{\textquotedbl} by Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado and Jeffrey Dean.},
 author = {Goldberg, Yoav and Levy, Omer},
 date = {15.02.2014},
 title = {word2vec Explained: deriving Mikolov et al.'s negative-sampling  word-embedding method},
 url = {https://arxiv.org/pdf/1402.3722},
 file = {Goldberg, Levy 15.02.2014 - word2vec Explained:C\:\\Users\\Doo5i\\Documents\\Citavi 6\\Projects\\TemporalGraphEmbedding\\Citavi Attachments\\Goldberg, Levy 15.02.2014 - word2vec Explained.pdf:pdf}
}

% This file was created with Citavi 6.7.0.0

@article{Goyal.2018,
 abstract = {Graphs, such as social networks, word co-occurrence networks, and communication networks, occur naturally in various real-world applications. Analyzing them yields insight into the structure of society, language, and different patterns of communication. Many approaches have been proposed to perform the analysis. Recently, methods which use the representation of graph nodes in vector space have gained traction from the research community. In this survey, we provide a comprehensive and structured analysis of various graph embedding techniques proposed in the literature. We first introduce the embedding task and its challenges such as scalability, choice of dimensionality, and features to be preserved, and their possible solutions. We then present three categories of approaches based on factorization methods, random walks, and deep learning, with examples of representative algorithms in each category and analysis of their performance on various tasks. We evaluate these state-of-the-art methods on a few common datasets and compare their performance against one another. Our analysis concludes by suggesting some potential applications and future directions. We finally present the open-source Python library we developed, named GEM (Graph Embedding Methods, available at https://github.com/palash1992/GEM), which provides all presented algorithms within a unified interface to foster and facilitate research on the topic.},
 author = {Goyal, Palash and Ferrara, Emilio},
 year = {2018},
 title = {Graph Embedding Techniques, Applications, and Performance: A Survey},
 url = {https://arxiv.org/pdf/1705.02801},
 pages = {78--94},
 volume = {151},
 issn = {09507051},
 journal = {Knowledge-Based Systems},
 doi = {10.1016/j.knosys.2018.03.022},
 file = {Goyal, Ferrara 2018 - Graph Embedding Techniques:C\:\\Users\\Doo5i\\Documents\\Citavi 6\\Projects\\TemporalGraphEmbedding\\Citavi Attachments\\Goyal, Ferrara 2018 - Graph Embedding Techniques.pdf:pdf}
}


@proceedings{Kraft.op.2003,
 year = {op. 2003},
 title = {CIKM 2003: Proceedings of the Twelfth ACM International Conference on Information {\&} Knowledge Management : November 3-8, 2003, New Orleans, Louisiana, USA},
 address = {New York, N.Y.},
 publisher = {{Association for Computing Machinery}},
 isbn = {1581137230},
 editor = {Kraft, Donald and Frieder, Ophir and Hammer, Joachim and Qureshi, Sajda and Seligman, Len},
 doi = {10.1145/956863}
}


@inproceedings{LibenNowell.op.2003,
 author = {Liben-Nowell, David and Kleinberg, Jon},
 title = {The link prediction problem for social networks},
 pages = {556},
 publisher = {{Association for Computing Machinery}},
 isbn = {1581137230},
 editor = {Kraft, Donald and Frieder, Ophir and Hammer, Joachim and Qureshi, Sajda and Seligman, Len},
 booktitle = {CIKM 2003},
 year = {op. 2003},
 address = {New York, N.Y.},
 doi = {10.1145/956863.956972},
 file = {Liben-Nowell, Kleinberg 2003 - The link prediction problem:C\:\\Users\\Doo5i\\Documents\\Citavi 6\\Projects\\TemporalGraphEmbedding\\Citavi Attachments\\Liben-Nowell, Kleinberg 2003 - The link prediction problem.pdf:pdf}
}

% This file was created with Citavi 6.7.0.0

@inproceedings{Ding.2001,
 author = {Ding, C.H.Q. and He, Xiaofeng and Zha, Hongyuan and Gu, Ming and Simon, H. D.},
 title = {A min-max cut algorithm for graph partitioning and data clustering},
 pages = {107--114},
 publisher = {{IEEE Computer Society}},
 isbn = {0-7695-1119-8},
 editor = {Cercone, Nick},
 booktitle = {Proceedings},
 year = {2001},
 address = {Los Alamitos, Calif},
 doi = {10.1109/ICDM.2001.989507},
 file = {https://ieeexplore.ieee.org/document/989507}
}


@misc{Bruss.03.07.2019,
 abstract = {Graph embedding is a popular algorithmic approach for creating vector representations for individual vertices in networks. Training these algorithms at scale is important for creating embeddings that can be used for classification, ranking, recommendation and other common applications in industry. While industrial systems exist for training graph embeddings on large datasets, many of these distributed architectures are forced to partition copious amounts of data and model logic across many worker nodes. In this paper, we propose a distributed infrastructure that completely avoids graph partitioning, dynamically creates size constrained computational graphs across worker nodes, and uses highly efficient indexing operations for updating embeddings that allow the system to function at scale. We show that our system can scale an existing embeddings algorithm - skip-gram - to train on the open-source Friendster network (68 million vertices) and on an internal heterogeneous graph (50 million vertices). We measure the performance of our system on two key quantitative metrics: link-prediction accuracy and rate of convergence. We conclude this work by analyzing how a greater number of worker nodes actually improves our system's performance on the aforementioned metrics and discuss our next steps for rigorously evaluating the embedding vectors produced by our system.},
 author = {Bruss, C. Bayan and Khazane, Anish and Rider, Jonathan and Serpe, Richard and Nagrecha, Saurabh and Hines, Keegan E.},
 date = {03.07.2019},
 title = {Graph Embeddings at Scale},
 url = {https://arxiv.org/pdf/1907.01705},
 file = {Bruss, Khazane et al. 03.07.2019 - Graph Embeddings at Scale:C\:\\Users\\Doo5i\\Documents\\Citavi 6\\Projects\\TemporalGraphEmbedding\\Citavi Attachments\\Bruss, Khazane et al. 03.07.2019 - Graph Embeddings at Scale.pdf:pdf}
}

% This file was created with Citavi 6.7.0.0

@misc{Cai.22.09.2017,
 abstract = {Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation and space cost. Graph embedding is an effective yet efficient way to solve the graph analytics problem. It converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximally preserved. In this survey, we conduct a comprehensive review of the literature in graph embedding. We first introduce the formal definition of graph embedding as well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work address these challenges in their solutions. Finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques and application scenarios.},
 author = {Cai, Hongyun and Zheng, Vincent W. and Chang, Kevin Chen-Chuan},
 date = {22.09.2017},
 title = {A Comprehensive Survey of Graph Embedding: Problems, Techniques and  Applications},
 url = {https://arxiv.org/pdf/1709.07604},
 file = {Cai, Zheng et al. 22.09.2017 - A Comprehensive Survey of Graph (2):C\:\\Users\\Doo5i\\Documents\\Citavi 6\\Projects\\TemporalGraphEmbedding\\Citavi Attachments\\Cai, Zheng et al. 22.09.2017 - A Comprehensive Survey of Graph (2).pdf:pdf}
}
% This file was created with Citavi 6.7.0.0

@misc{Rong.11.11.2014,
 abstract = {The word2vec model and application by Mikolov et al. have attracted a great amount of attention in recent two years. The vector representations of words learned by word2vec models have been shown to carry semantic meanings and are useful in various NLP tasks. As an increasing number of researchers would like to experiment with word2vec or similar techniques, I notice that there lacks a material that comprehensively explains the parameter learning process of word embedding models in details, thus preventing researchers that are non-experts in neural networks from understanding the working mechanism of such models.  This note provides detailed derivations and explanations of the parameter update equations of the word2vec models, including the original continuous bag-of-word (CBOW) and skip-gram (SG) models, as well as advanced optimization techniques, including hierarchical softmax and negative sampling. Intuitive interpretations of the gradient equations are also provided alongside mathematical derivations.  In the appendix, a review on the basics of neuron networks and backpropagation is provided. I also created an interactive demo, wevi, to facilitate the intuitive understanding of the model.},
 author = {Rong, Xin},
 date = {11.11.2014},
 title = {word2vec Parameter Learning Explained},
 url = {https://arxiv.org/pdf/1411.2738},
 file = {Rong 11.11.2014 - word2vec Parameter Learning Explained:C\:\\Users\\Doo5i\\Documents\\Citavi 6\\Projects\\TemporalGraphEmbedding\\Citavi Attachments\\Rong 11.11.2014 - word2vec Parameter Learning Explained.pdf:pdf}
}
















