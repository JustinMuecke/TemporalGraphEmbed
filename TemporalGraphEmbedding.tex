%% This is file `sample-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}
%% NOTE that a single column version may be required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen,review]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.


\setcopyright{acmcopyright}
\copyrightyear{2021}
\acmYear{2021}
\acmDOI{}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Seminar Data Science]{Ulm: Seminar Data-Science}{20/21}{Ulm, Germany}
\acmBooktitle{University Ulm: Seminar Data Science}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}
\usepackage{tikz}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Temporal Graph Embedding}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Justin Mücke}
\email{justin.muecke@uni-ulm.com}
\affiliation{%
  \institution{University Ulm}
  \streetaddress{Albert-Einstein-Allee}
  \city{Ulm}
  \country{Germany}
  \postcode{89077}
}



%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Justin Mücke}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  
\end{abstract}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Temporal Graph, Embedding}

%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
ML finden Graphen ganz blöd! Drum machen wir jetzt vektoren \(uwu\)

\section{Preliminary}
To start this paper out, let's first create a level playing field for all readers by laying out what exactly a Graph is, 
how it can change over time and what is meant when we talk about embedding.



\subsection{Graphs}
A graph is a mathematical construct which is used in a variety of tasks. It is often used to model relationships between 
entities, thus making it possible to operate on such structures to, for example, analyze them.\\
\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}
      \draw (0,-0.5) node [circle,inner sep=0pt,draw] {EO};
      \draw (1.75,-1) node [circle,inner sep=0pt,draw] {JM};
      \draw (0.5,-2) node [circle,inner sep=0pt,draw] {OK};
      \draw (3,-0.5) node [circle,inner sep=0pt,draw] {YH};
      \draw (2.75,-1.75) node [circle,inner sep=0pt,draw] {PB};

      \draw (0.27, -0.56) -- (1.49, -0.95);
      \draw (1.55, -1.17) -- (0.7, -1.8);
      \draw (2, -0.95) -- (2.75, -0.65);
      \draw (1.95, -1.17) -- (2.55, -1.6);
    \end{tikzpicture}
\end{center}
\caption{Part of the follower-relationship-graph of personal Instagram account.}
\end{figure}
 Mathematically it is consisting of two sets \(G = (V, E)\), where V consists of all the vertices of the Graph, and E of all the edges, represented through a tuple of two vertices.
 Our graph above would thus look like following: \\
 \( G_i = (\{E,O,Y,P,J\}, \{(J,E),(J,O), (J,P), (J,Y)\}) \).

\subsection{Temporal Graphs}
A temporal graph, on the other hand, is a graph that changes its structure over time. This happens when either one of our sets \((V,E)\) changes. %%Zweiter Satz zu umgs.
Our graph \(G\) is then represented through \(G = \{g_1, g_2, \ldots, g_t\}\) where \(g_i\) is the static graph after time \(i*\Delta t\).
\cite{DBLP:journals/corr/abs-1905-05304}
%Umschreiben G={G1.... Gn} ist evolution des graphen


\subsection{Embedding}
Embedding is a process in which the graph \(G = (V_g, E_g)\) is transformed into a set of vectors \(V\).
These capture the graph topology, vertex-to-vertex relationship as well as other relevant information.
This way, it is more accessible for analyzing the graph and comparing it to others.
Overall we can divide graph embedding techniques into two categories; 
vertex embedding and graph embedding. \\
When using vertex embedding, there has to be one vector \(v\) for each node in \(G\), so that \(|V| = |V_g|\). This is used to make prediction 
on a node level.\\
When using graph embedding, there is one vector representing the whole graph. This method is most useful to analyze the data on a graph level e.g.
comparing two graph structures with each other.\\
In order to achieve this transformation there are several methods available.

\subsubsection{Word2Vec}
The first method is word2vec which is the foundation for many other methods. 
It takes a text \(T = (w_1, w_2, \ldots, w_n)\) as input and returns a set of vectors\(V\), where \(v_i\) is a distribution describing how likely it is for each word 
to be a direct neighbor to it. The neighbors of a word \(w_c\) are defined as \(\{w_i | i \in (c-\epsilon, c+\epsilon)\}\) where \(\epsilon\) describes a predefined window size.
With this, the whole text is represented solely through vectors, where similar words have similar vectors.
One way to achieve this, is to train a neural network to create the distributions. This neural network consists of one input-, one hidden- and one output layer. 
Each word in our text is assigned an ID through a one-hot coded vector, so that each word \(w_i\) can be represented like \((0_1, 0_2, \ldots, 0_{i-1}, 1_i, 0_{i+1}, \ldots 0_n)\).
It then computes how likely it is for each word to be its neighbor and transforms it into a distribution using a softmax-function\cite{gao2018properties}.
\begin{figure}[h]
  \begin{center}
    \scalebox{0.8}{
    \begin{tikzpicture}
      \draw (0,0) rectangle (2,1) node[pos=.5] {Philosopers};  
      \draw (2,0) rectangle (3,1) node[pos=.5] {have};  
      \draw [fill=gray](3,0) rectangle (4.5,1) node[pos=.5] {debated};  
      \draw [fill=lightgray](4.5,0) rectangle (6,1) node[pos=.5] {Hume's};  
      \draw [fill=gray](6,0) rectangle (7.5,1) node[pos=.5] {problem}; 

      \draw (0,1.3) rectangle (2,2.3) node[pos=.5] {Philosopers};  
      \draw [fill=gray](2,1.3) rectangle (3,2.3) node[pos=.5] {have};  
      \draw [fill=lightgray](3,1.3) rectangle (4.5,2.3) node[pos=.5] {debated};  
      \draw [fill=gray](4.5,1.3) rectangle (6,2.3) node[pos=.5] {Hume's};  
      \draw (6,1.3) rectangle (7.5,2.3) node[pos=.5] {problem}; 

      \draw [fill=gray](0,2.6) rectangle (2,3.6) node[pos=.5] {Philosopers};  
      \draw [fill=lightgray](2,2.6) rectangle (3,3.6) node[pos=.5] {have};  
      \draw [fill=gray](3,2.6) rectangle (4.5,3.6) node[pos=.5] {debated};  
      \draw (4.5,2.6) rectangle (6,3.6) node[pos=.5] {Hume's};  
      \draw (6,2.6) rectangle (7.5,3.6) node[pos=.5] {problem}; 

      \draw [fill=lightgray](0,3.9) rectangle (2,4.9) node[pos=.5] {Philosopers};  
      \draw [fill=gray](2,3.9) rectangle (3,4.9) node[pos=.5] {have};  
      \draw (3,3.9) rectangle (4.5,4.9) node[pos=.5] {debated};  
      \draw (4.5,3.9) rectangle (6,4.9) node[pos=.5] {Hume's};  
      \draw (6,3.9) rectangle (7.5,4.9) node[pos=.5] {problem}; 

    \end{tikzpicture}}
  \end{center}
  \caption{Light-gray word, with dark-gray neighbor and \(\epsilon = 1\).}
\end{figure}

\subsubsection{DeepWalk}
This method is a continuation of the word2vec approach, but now the input is not a text, but a graph \(G = (V,E)\)
It consists of three steps. The first being the sampling, where random walks are performed from each node.
Hereby, a random walk is a path in the graph from a starting point \(v_i\) of defined length \(\lambda\). 
The following nodes are generated by: 
\begin{center}
  \(P(v_i = x|v_{i-1} = y) = \begin{cases} \frac{\pi_{xy}}{Z} &\text{if} (x,y) \in E \\ 0 & \text{otherwise} \end{cases}\)
\end{center} 
where \(\pi_{xy}\) is the probability between nodes \(x\) and \(y\) with\(Z\) as the normalizing constant.\\
It is sufficient to perform \(32-64\) random walks per Node. The resulting paths are of the same structure as sentences in a text, so 
the word2vec method can now be used on it.
The result is a set of vectors \(V\) where each vector \(v_i\) is a distribution of the probability that two nodes are next to each other.


\subsubsection{Node2Vec}
This is an optimized version of DeepWalk.
Again, there is the sampling phase in the beginning. The difference in the methods is, that in node2vec the random walks are now biased and not completely random.
An order for the random walk is defined through two parameters \(p\) and \(q\). To evaluate which node to visit next the walk looks at the transition probability
\(\pi_{xy}\) on the edge \((x,y)\). This probability is now defined as \(\pi_{xy} = \alpha_{pq}(x, y) * w_{xy}\) where \(w_{xy}\) is the weight assigned to the edge \((x,y)\) and:
\begin{center}
 \(\alpha_{pq}(xy) = \begin{cases} \frac{1}{p} & \text{if } d_{xy} = 0 \\ 1 & \text{if } d_{xy} = 1 \\\frac{1}{q} & \text{if } d_{xy} = 2 \\\end{cases}\)
\end{center}
with \(d_{xy}\) being the shortest path between nodes \(x\) and \(y\).
Hereby, our parameter \(p\) defines how likely it is to revisit a node of the walk. Setting \(p\) to a higher value encourages the walk to go deeper into the graph, whereas a small value 
ensures the walk to stay local around the starting point. 
In contrast, \(q\) describes the preference of nearer or further nodes. For \(q > 1\) the walk favors closer nodes, for \(q < 1\) nodes that are further away.\cite{Grover.03.07.2016}

\subsubsection{Structural Deep Network Embedding}
In contrast to the methods used before, SDNE does not use random walks. It aims to preserve local pairwise similarity which characterizes the local structure, and 
as well as the global network structure.
To achieve this, we use two autoencoder neural network. These get an adjacency vector as input and want to construct node adjacency as output.
We then compute the distance between the two outputs and add it to the loss function of the network.
The total loss function is then computed through summation of the distance loss plus the losses of the two encoders.
At the end we remain with a collection of adjacency vectors which describe the graph structure.

\subsubsection{Graph2Vec}
Now we don't want to represent the nodes as vectors, but the whole graph. For that we have once again three steps. 
In the first step we create sub-graphs for each node and encode then once again in a one-hot code. We then use these sub-graphs to train the network used in 
word2vec to maximize the probability that a predicted sup-graph exists in the input graph. The embedding is then the result of the network.
\cite{Godec.31.12.2018}



\section{Embedding of Temporal Graphs}
Let \(G = (V,E)\) be a temporal graph with \(G_t=\{G_{t_1}, \ldots,G_{t_T}\} \) as its evolution over the time steps \(T\).
The goal is now, to create a vector space which can be used to analyze the graph to find anomalies in it, compare it to other graphs, classify nodes or predict if 
certain links are going to exist.
Depending on which outcome is wanted, different methods have to be used to optimize it. For predictions on the node level like node classification and link prediction a node-level algorithm should be used. 
On the other hand, a graph-level algorithm should be used to compare graphs with each other and to find anomalies, as shown in 3.3.

\subsection{tNodeEmbed}
As aforementioned, a node-level algorithm should be used to perform e.g. link predictions and node classification. 
One proposed algorithm of that form is \emph{tNodeEmbed}. Here, the embedding can be split into three parts: 
%% Work up: initialisierung -> auslenkung -> zusammenfassung
\subsubsection{Initialization}
To start out, the algorithm initializes a representative vector \(Q_t \in \mathbb{R}^{T \times d}\) for each node \(v\) in all graphs \(G_{t_1}, \ldots, G_{t_T} \), where \(d\) is the embedding size, and \(T\) the number of time steps.
These vectors are created using the node2vec algorithm discussed in 2.3.3.

\subsubsection{Node alignment}
%Vektors are being combined into matrizes and then rotated to align
A downside to node2vec is that, when running on a graph, it aims to minimize the word embedding distances but not consistency over multiple trainings.
The resulting embedding could be in a two-dimensional space for example. It lays in a x-y-plane. Now imagine a rotation around the unused z-axis. The resulting graphs have the
same structure but different orientation in the plane, thus not having an alignment, as seen in figure 3.
%Besser ausführen -> Bildchen

Similarly, when embedding two graphs \(G_{t_i}\) and \(G_{t_j}\) it is not guaranteed that, even if the graphs are identical, their axes align.
To adjust those embeddings, tNodeEmbed uses an orthogonal transformation between embeddings at two time points \(t_i\) and \(t_j\)\cite{Schonemann.1966}.
Used in this algorithm, it takes the matrix \(Q_t \in \mathbb{R}^{d\times |V|}\) of node embeddings at time \(t\). Then the matrices get aligned iteratively starting from the earliest timestep.
For the alignment an orthogonal matrix \(R\) between \(Q_t\) and \(Q_{t+1}\) is needed to result in the final embedding \(Q'_t = RQ_t\).
The matrix are is approximated by : 
\begin{center}
  \(R_{t+1} = \argmin_{R s.t. R^TR =I} \|RQ_{t+1} - Q_t\| \)
\end{center}
where \(R_{t+1} \in \mathbb{R}^{d \times d} \) is the transformation which fits the time steps the most.
\begin{figure}[h]
  \includegraphics[scale=0.1]{UnalignedGraphs.png}
  \caption{a) graph described by first embedding. b) graph described by second embedding. The red node do not align even though the graph structure is the same.}
\end{figure}

\subsubsection{Finalization}
The algorithm works in such a way, that the only difference in the algorithm, between the two outcomes, is the loss function. 
For the node classification task it uses a categorical cross-entropy loss:
\begin{center}
 \(L_{task} = -\sum_{v\in V} \log {Pr(class(v)|f_T(v))}\)
\end{center}
For the link prediction task it considers a binary classification loss: 
\begin{center}
  \(L_{task} = -\sum_{v_1, v_2 \in V} \log{Pr((v_1,v_2)\in E | g(f_T(v_1), f_T(v_2)))}\)
\end{center}
The Algorithm now wants to learn a function \(F_T\) so that \(f_T(v) = F_T(v,G_1, \ldots, G_T)\) that optimizes for \(L_Task\).
The function is defined recursively as: 
\begin{center}
  \(f_{t+1} = \sigma(Af_t(v) + BQ_tR_tv)\) \\
  and \(f_0(v) = \vec{0}\)
\end{center}
with \(A,B,R_t\) and \(Q_t\) being matrices, \(v\) again being a one-hot vector representing a node and \(\sigma\) being an activation funcion.
The final temporal embedding is now defined as the minimization of the loss-function:
\begin{center}
  \( L = \min_{A,B,Q_1, \ldots Q_T,R2, \ldots RT} L_{task}\).
\end{center}
Now, it is only a Question on how to define \(A,B\) and \(\sigma\).
Due to the steps beforehand, each node is associated with a matrix \(X^{(v)} \in \mathbb{R}^{T \times d}\) consisting of 
all embeddings \(T\), which are of size\(d\), thus the whole graph can be seen as \(G_X = X^{(v_1)},\ldots, X^{(v_{|V|})}\).
To perform the wanted graph-prediction tasks, the matrices representing each node now need to be reduced into a single vector, so it can be used as input for a classifier, as in the static embeddings.
For this it is proposed to use recurrent neural networks with long short term memory.
\cite{Singer.2019}


\subsection{tdGraphEmbed}
Starting paper
\cite{Beladev.2020}


\section{Comparison}
How do Methods differ -> nodelevel / Graphlevel?

\section{Application}
Why do we use Embedding
\subsection{Similarity}
Differences Between graphs (exp - googletrends)
\subsection{Anomaly}
Where does it differ

\section{Conclusion}
%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{IEEEtr}
\bibliography{citation}
%% If your work has an appendix, this is the place to put it.

\end{document}
%%
%% End of file `sample-authordraft.tex'.
